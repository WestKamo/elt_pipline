ğŸ­ The Containerized Data Factory
A Full-Stack ELT Pipeline built with Docker, PostgreSQL, dbt, and Apache Airflow.

Welcome to my data engineering playground! What started as a simple python script to move data between two databases has evolved into a robust, orchestrated data platform. I built this to master the fundamentals of modern data infrastructureâ€”handling everything from raw container networking to complex transformation logic.

ğŸš€ What's Under the Hood?
This isn't just a copy-paste project. It's a fully integrated stack simulating a real-world data environment.

ğŸ³ Docker & Docker Compose: The backbone. Everything runs in isolated containers with a custom internal network (elt_network).

ğŸ˜ PostgreSQL (x2): A "Source" DB (simulating a production app) and a "Destination" DB (our Data Warehouse).

ğŸ Python: A custom ELT script acting as the "delivery driver," extracting raw data and loading it into the warehouse.

âœ¨ dbt (Data Build Tool): The transformation layer. Once data arrives, dbt cleans it, joins tables, and runs tests to ensure quality.

ğŸŒ¬ï¸ Apache Airflow: The orchestrator. Instead of fragile cron jobs, I use Airflow DAGs and DockerOperators to manage dependencies and schedule the entire workflow.

ğŸ› ï¸ The Architecture
Extraction & Loading (EL):

A custom Python script waits for the source_db to initialize.

It uses pg_dump and psql to stream data directly to the destination_db.

Challenge Solved: Optimized for WSL2 performance using Named Volumes to prevent I/O bottlenecks and timeouts.

Transformation (T):

dbt takes over once the data lands.

configured sources.yml to treat the destination DB as the source of truth (solving cross-database querying limitations).

Includes custom SQL models (stg_users, film_ratings_summary), generic tests, and Jinja macros for reusable logic.

Orchestration:

Airflow manages the lifecycle.

It spins up the Python container â¡ï¸ Waits for success â¡ï¸ Spins up the dbt container.

If any step fails, the pipeline stops and alerts (no more silent failures!).

ğŸ“‚ Repository Structure
Plaintext

custom-elt-project-main/
â”œâ”€â”€ dags/                  # Airflow DAGs (The "Manager" scripts)
â”‚   â””â”€â”€ elt_dag.py         # Defines the task order: Python Script -> dbt
â”œâ”€â”€ custom_postgres/       # The dbt Project (The "Chef")
â”‚   â”œâ”€â”€ models/            # SQL Transformations (Staging & Marts)
â”‚   â”œâ”€â”€ macros/            # Reusable SQL functions
â”‚   â””â”€â”€ profiles.yml       # Connection configs
â”œâ”€â”€ elt_script/            # The Extraction Script (The "Driver")
â”‚   â”œâ”€â”€ Dockerfile         # Custom image definition
â”‚   â””â”€â”€ elt_script.py      # The logic to move data
â”œâ”€â”€ source_db_init/        # Initial seed data for the source DB
â””â”€â”€ docker-compose.yaml    # The Master Blueprint
âš¡ How to Run It
Prerequisites: Docker and Docker Compose.

Clone the repo:

Bash

git clone <your-repo-url>
cd custom-elt-project-main
The "WSL Hack" (If you are on Windows/WSL2): Airflow needs permission to talk to the Docker daemon.

Bash

sudo chmod 666 /var/run/docker.sock
Launch the Stack: Build the images and spin up the containers.

Bash

docker-compose up -d --build
Access the Airflow UI:

Go to: http://localhost:8080

User: airflow | Pass: airflow

Turn the toggle ON for elt_and_dbt_pipeline and click the Play button.

Watch the Magic: Go to the Graph View in Airflow. You'll see the system automatically:

âœ… Provision a container to move data.

âœ… Provision a container to run dbt transformations.

âœ… Mark the job as "Success".

ğŸ§  Key Learnings & Features
Race Conditions: Solved the "chicken and egg" problem where the script would run before the database was ready using Airflow dependencies (and Healthchecks).

Data Quality: Implemented dbt tests to catch null values and referential integrity issues before they hit the dashboard.

Infrastructure as Code: The entire environmentâ€”from database credentials to Airflow connectionsâ€”is defined in code, making it reproducible on any machine.

Built with â¤ï¸ and a lot of coffee by [Phindile Ivy].
