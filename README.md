# ðŸ­ The Containerized Data Factory

**A Full-Stack ELT Pipeline built with Docker, PostgreSQL, dbt, and Apache Airflow.**

Welcome to my data engineering playground! What started as a simple Python script to move data between two databases has evolved into a robust, orchestrated data platform. I built this to master the fundamentals of modern data infrastructureâ€”handling everything from raw container networking to complex transformation logic.

---

## ðŸš€ What's Under the Hood?

This isn't just a copy-paste project. It is a fully integrated stack simulating a real-world data environment.

* **ðŸ³ Docker & Docker Compose:** The backbone. Everything runs in isolated containers with a custom internal network (`elt_network`).
* **ðŸ˜ PostgreSQL (x2):** A "Source" DB (simulating a production app) and a "Destination" DB (our Data Warehouse).
* **ðŸ Python:** A custom ELT script acting as the "delivery driver," extracting raw data and loading it into the warehouse.
* **âœ¨ dbt (Data Build Tool):** The transformation layer. Once data arrives, dbt cleans it, joins tables, and runs tests to ensure quality.
* **ðŸŒ¬ï¸ Apache Airflow:** The orchestrator. Instead of fragile cron jobs, I use Airflow DAGs and `DockerOperators` to manage dependencies and schedule the entire workflow.

---

## ðŸ› ï¸ The Architecture

### 1. Extraction & Loading (EL)
* A custom Python script waits for the `source_db` to initialize.
* It uses `pg_dump` and `psql` to stream data directly to the `destination_db`.
* **Challenge Solved:** Optimized for WSL2 performance using **Named Volumes** to prevent I/O bottlenecks and timeouts.

### 2. Transformation (T)
* **dbt** takes over once the data lands.
* Configured `sources.yml` to treat the destination DB as the source of truth (solving cross-database querying limitations).
* Includes custom SQL models (`stg_users`, `film_ratings_summary`), generic tests, and Jinja macros for reusable logic.

### 3. Orchestration
* **Airflow** manages the lifecycle.
* It spins up the Python container âž¡ï¸ Waits for success âž¡ï¸ Spins up the dbt container.
* If any step fails, the pipeline stops and alerts (no more silent failures!).

---

## ðŸ“‚ Repository Structure

```text
custom-elt-project-main/
â”œâ”€â”€ dags/                  # Airflow DAGs (The "Manager" scripts)
â”‚   â””â”€â”€ elt_dag.py         # Defines the task order: Python Script -> dbt
â”œâ”€â”€ custom_postgres/       # The dbt Project (The "Chef")
â”‚   â”œâ”€â”€ models/            # SQL Transformations (Staging & Marts)
â”‚   â”œâ”€â”€ macros/            # Reusable SQL functions
â”‚   â””â”€â”€ profiles.yml       # Connection configs
â”œâ”€â”€ elt_script/            # The Extraction Script (The "Driver")
â”‚   â”œâ”€â”€ Dockerfile         # Custom image definition
â”‚   â””â”€â”€ elt_script.py      # The logic to move data
â”œâ”€â”€ source_db_init/        # Initial seed data for the source DB
â””â”€â”€ docker-compose.yaml    # The Master Blueprint
